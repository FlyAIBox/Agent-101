{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 使用 LoRA 进行参数高效微调\n",
        "\n",
        "- 功能：\n",
        "  - 自动准备 SMS Spam 数据集并构建数据加载器\n",
        "  - 加载 GPT-2 基座，替换为分类头\n",
        "  - 用 LoRA 递归替换 Linear 层，冻结基座，仅训练 LoRA 参数\n",
        "  - 训练、可视化与评估（训练/验证/测试准确率）\n",
        "\n",
        "- 目的：\n",
        "  - 面向大模型技术初学者，帮助快速理解 LoRA 原理与落地流程\n",
        "  - 在小显存/低算力条件下实现高效微调，掌握 rank/alpha 等关键超参\n",
        "  - 学会将通用语言模型适配为特定下游任务（短信垃圾分类）\n",
        "\n",
        "- 效果：\n",
        "  - 以 SMS 垃圾短信为例，仅训练少量 LoRA 参数（如 rank=16 时为数百万级），参数量较全量微调显著降低\n",
        "  - 训练 3–5 个 epoch 即可将准确率从约 45–50% 提升至 95%+（示例效果，随随机种子与环境略有波动）\n",
        "  - LoRA 的 B 初始化为 0，替换后初始性能与替换前一致；部署时只需叠加 LoRA 增量参数\n",
        "\n",
        "提示：如需 GPU，建议在支持 CUDA 的环境执行。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 依赖安装（按需执行，已安装可跳过）\n",
        "# 仅保留本笔记必需依赖：torch, tiktoken, numpy, pandas, matplotlib\n",
        "# 注意：numpy 约束为 [1.26, 2.1) 以兼容 torch/pandas 等\n",
        "\n",
        "%pip install -U pip\n",
        "%pip install -U \"numpy>=1.26,<2.1\" \"pandas>=2.2.1\" \"matplotlib>=3.7.1\" \"tiktoken>=0.5.1\"\n",
        "\n",
        "# PyTorch（二选一，按需取消注释；均满足 >=2.3.0）\n",
        "# CPU 版：\n",
        "# %pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cpu\n",
        "# CUDA 12.1 版（示例）：\n",
        "# %pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 安装书本配套工具（提供 previous_chapters, gpt_download 等）\n",
        "%pip install -U git+https://github.com/rasbt/LLMs-from-scratch.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 占位单元（可忽略）\n",
        "# 说明：该单元原为重复安装命令，已合并到上方“依赖安装”单元。\n",
        "# 若无需使用，可保持为空或删除此单元。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 版本检查（可选）：便于复现实验环境与排查\n",
        "# 仅检查本笔记所需依赖；确保版本满足上方安装约束\n",
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"numpy\",      # 科学计算，需 >=1.26,<2.1\n",
        "    \"pandas\",     # 表格处理，需 >=2.2.1\n",
        "    \"matplotlib\", # 可视化，需 >=3.7.1\n",
        "    \"tiktoken\",   # GPT-2 分词器，需 >=0.5.1\n",
        "    \"torch\"       # PyTorch，需 >=2.3.0（CPU/GPU 二选一安装）\n",
        "]\n",
        "for p in pkgs:\n",
        "    try:\n",
        "        print(f\"{p} version: {version(p)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{p} not installed: {e}\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## E.1 LoRA 简介（概念层）\n",
        "\n",
        "- 传统微调：学习完整权重更新 ΔW，使得 `W_updated = W + ΔW`。\n",
        "- LoRA：用更小的矩阵分解近似 ΔW，即 `ΔW ≈ A B`，因此 `W_updated = W + A B`。\n",
        "- 前向时可写作 `x(W + AB) = xW + xAB`，无需改写原权重 W，AB 作为“增量分支”动态叠加。\n",
        "- 超参数：\n",
        "  - rank（秩）：控制 A、B 的内维度，决定新增可训练参数量与适配能力。\n",
        "  - alpha：缩放系数，控制 LoRA 分支对原层输出的影响强度。\n",
        "- 实践意义：只训练少量参数（A、B），显著降低显存与训练时间。\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## E.2 数据集准备（SMS Spam）\n",
        "\n",
        "我们使用公开数据集 `SMSSpamCollection`，包含短信文本及其标签（ham=正常，spam=垃圾）。下方代码将自动下载、解压，并划分训练/验证/测试集。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据下载与拆分（与原书 ch06 逻辑一致）\n",
        "# 步骤说明：\n",
        "# 1) 下载并解压 SMS Spam 数据集；\n",
        "# 2) 读取 TSV 文件为 DataFrame，并将标签 ham/spam 映射为 0/1；\n",
        "# 3) 使用辅助函数做类别平衡与划分训练/验证/测试集；\n",
        "# 4) 将划分结果保存为 CSV，方便后续 Dataset 加载。\n",
        "\n",
        "import urllib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# 依赖于项目内的辅助函数；若不可用，可切换到配套包 llms_from_scratch（见安装单元）\n",
        "from previous_chapters import (\n",
        "    download_and_unzip_spam_data,\n",
        "    create_balanced_dataset,\n",
        "    random_split\n",
        ")\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "try:\n",
        "    # 主下载地址\n",
        "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
        "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
        "    # 失败则回退到备用地址\n",
        "    print(f\"主 URL 失败: {e}. 尝试备用地址...\")\n",
        "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
        "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
        "\n",
        "# 读取、平衡、划分\n",
        "# 原始文件为制表符分隔，列为 Label, Text\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "# 保证类别分布更均衡，避免训练时偏斜\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "# 标签映射：ham->0, spam->1，便于后续计算损失\n",
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# 按 7:2:1（训练:测试:验证）比例划分\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "\n",
        "# 落盘，便于复用与调试\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)\n",
        "\n",
        "print(\"数据已准备：train.csv / validation.csv / test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 构建 Dataset 与 DataLoader\n",
        "# 说明：\n",
        "# - 使用 GPT-2 的 BPE 分词器（tiktoken.get_encoding(\"gpt2\")）。\n",
        "# - 训练集初始化时不设 max_length（内部会统计训练集的最长长度），\n",
        "#   将该长度传给验证/测试，以保证三者序列长度一致，避免评估偏差。\n",
        "\n",
        "import torch\n",
        "import tiktoken\n",
        "from previous_chapters import SpamDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 初始化 GPT-2 分词器\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 构建三份数据集；验证/测试共用训练集统计出的 max_length\n",
        "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
        "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
        "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
        "\n",
        "# DataLoader 参数：\n",
        "# - batch_size: 每批样本数；\n",
        "# - shuffle: 训练集需打乱；\n",
        "# - drop_last: 训练时丢弃最后不足一批的数据，便于批归一；\n",
        "# - num_workers: 数据加载子进程数（Windows/笔记本环境可设 0）。\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)  # 固定随机种子，保证结果可复现\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "print(f\"训练批次数: {len(train_loader)} | 验证批次数: {len(val_loader)} | 测试批次数: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## E.3 初始化 GPT-2 并改为分类任务\n",
        "\n",
        "步骤：\n",
        "1. 下载并加载 GPT-2 权重到自定义 `GPTModel`。\n",
        "2. 将语言建模头替换为二分类头（输出维度=2）。\n",
        "3. 验证加载是否正常：让模型生成一段文本观察输出是否通顺。\n",
        "4. 查看未微调前的基线准确率。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载 GPT-2 预训练模型并改为分类任务\n",
        "# 说明：\n",
        "# - 使用项目提供的下载与装载函数，将公开的 GPT-2 预训练权重导入自定义 `GPTModel`。\n",
        "# - 将原本的语言建模输出头替换为二分类头（输出维度=2）。\n",
        "# - 通过一次简单文本生成做 sanity check，确认模型加载无误。\n",
        "# - 评估 LoRA 前的基线准确率（少量批次抽样），作为对比基准。\n",
        "\n",
        "from gpt_download import download_and_load_gpt2\n",
        "from previous_chapters import GPTModel, load_weights_into_gpt\n",
        "from previous_chapters import (\n",
        "    generate_text_simple,\n",
        "    text_to_token_ids,\n",
        "    token_ids_to_text,\n",
        "    calc_accuracy_loader\n",
        ")\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-small (124M)\"  # 可改为 medium/large/xl，但显存与下载时长会增加\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # GPT-2 词表大小\n",
        "    \"context_length\": 1024,  # 最大上下文长度\n",
        "    \"drop_rate\": 0.0,        # 先不加 dropout，便于对比\n",
        "    \"qkv_bias\": True         # 注意力 QKV 是否使用 bias\n",
        "}\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "# 下载权重并装载到自定义 GPTModel\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()\n",
        "\n",
        "# 替换输出头为二分类头\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
        "\n",
        "# 设备选择（优先使用 CUDA，其次 CPU）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 文本生成 sanity check（加载是否成功）\n",
        "text_1 = \"Every effort moves you\"\n",
        "ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        ")\n",
        "print(token_ids_to_text(ids, tokenizer))\n",
        "\n",
        "# 未微调前的基线准确率（抽样评估 10 批）\n",
        "torch.manual_seed(123)\n",
        "print(\"评估未微调基线（各取 10 批）...\")\n",
        "train_acc = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_acc = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_acc = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "print(f\"Train: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}% | Test: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## E.4 LoRA 实现与全模型替换\n",
        "\n",
        "我们实现两个类：\n",
        "- `LoRALayer`：构造 A、B 两个矩阵，前向返回 `alpha * (x @ A @ B)`；B 初始化为 0，确保替换后初始输出不变。\n",
        "- `LinearWithLoRA`：包装原 `Linear` 层，输出为 `linear(x) + lora(x)`。\n",
        "\n",
        "并提供 `replace_linear_with_lora(model, rank, alpha)` 递归替换函数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 实现 LoRA 层与线性替换层\n",
        "# 说明：\n",
        "# - LoRALayer 仅引入两个小矩阵 A、B，表示对原 Linear 权重的低秩近似更新 ΔW≈AB；\n",
        "#   训练时只优化 A、B，可显著减少可训练参数量。\n",
        "# - LinearWithLoRA 在不修改原权重的前提下，叠加 LoRA 增量（linear(x)+lora(x)）。\n",
        "# - replace_linear_with_lora 递归遍历模型，将所有 torch.nn.Linear 替换为 LinearWithLoRA。\n",
        "\n",
        "import math\n",
        "\n",
        "class LoRALayer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    中文说明：\n",
        "    - in_dim/out_dim：与被适配的 Linear 层输入/输出维度一致。\n",
        "    - rank：低秩分解的秩（内维度），越大表达能力越强，但参数更多。\n",
        "    - alpha：缩放系数，控制 LoRA 增量的强度。\n",
        "    - 初始化：A 用 Kaiming Uniform，B 用 0，保证替换后初始输出与原模型一致。\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim: int, out_dim: int, rank: int, alpha: float):\n",
        "        super().__init__()\n",
        "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
        "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # 前向：返回 LoRA 增量；B 初始为 0，首次替换不改变输出\n",
        "        return self.alpha * (x @ self.A @ self.B)\n",
        "\n",
        "class LinearWithLoRA(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    中文说明：\n",
        "    - 用 LoRA 包装原始 Linear：输出 = 原线性输出 + LoRA 增量。\n",
        "    - 仅训练 LoRA 的 A、B，原 Linear 的参数保持冻结（后续统一冻结）。\n",
        "    \"\"\"\n",
        "    def __init__(self, linear: torch.nn.Linear, rank: int, alpha: float):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(x) + self.lora(x)\n",
        "\n",
        "def replace_linear_with_lora(model: torch.nn.Module, rank: int, alpha: float):\n",
        "    \"\"\"递归替换模型中的所有 Linear 为 LinearWithLoRA\"\"\"\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
        "        else:\n",
        "            replace_linear_with_lora(module, rank, alpha)\n",
        "\n",
        "# 冻结原模型参数，仅训练 LoRA 分支\n",
        "print(f\"替换前可训练参数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "print(f\"冻结后可训练参数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# 执行替换（rank/alpha 可按需调整；rank 越大可训练参数越多，适配能力更强）\n",
        "replace_linear_with_lora(model, rank=16, alpha=16)\n",
        "\n",
        "# 统计当前可训练参数（此时只有 LoRA 分支，原 Linear 已冻结）\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"LoRA 可训练参数: {trainable:,}\")\n",
        "\n",
        "# 再把 LoRA 的 A、B 打开训练（其余参数保持冻结）\n",
        "for m in model.modules():\n",
        "    if isinstance(m, LoRALayer):\n",
        "        m.A.requires_grad = True\n",
        "        m.B.requires_grad = True\n",
        "\n",
        "print(\"已完成 LoRA 替换与参数冻结设置。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA 替换后，验证基线应基本不变\n",
        "# 原理：B 初始化为 0，LoRA 增量初始为 0，因此替换后但未训练前，性能应与替换前一致。\n",
        "from previous_chapters import calc_accuracy_loader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "print(\"替换后、训练前的抽样准确率（各取 10 批）...\")\n",
        "train_acc = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_acc = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_acc = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "print(f\"Train: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}% | Test: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 训练与评估\n",
        "\n",
        "- 仅优化 LoRA 分支（A、B），原模型权重保持冻结。\n",
        "- 学习率可从 5e-5 起步，epoch 取 3-5 做演示即可。\n",
        "- 训练中周期性评估训练/验证损失与准确率，确认收敛趋势。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from previous_chapters import train_classifier_simple\n",
        "\n",
        "# 仅训练需要梯度的参数（此时只包含 LoRA 的 A 与 B）\n",
        "optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"训练耗时: {(end_time - start_time)/60:.2f} 分钟\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 绘制训练/验证损失曲线（可选）\n",
        "from previous_chapters import plot_values\n",
        "\n",
        "import torch\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 全量评估（全数据集）\n",
        "from previous_chapters import calc_accuracy_loader\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Train: {train_accuracy*100:.2f}% | Val: {val_accuracy*100:.2f}% | Test: {test_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 经验与建议（面向初学者）\n",
        "\n",
        "- 何时用 LoRA：模型大、显存/时间受限、数据量不大时优先考虑（也可用 Q-LoRA 节省显存）。\n",
        "- 超参：`rank` 常用 4/8/16/32；`alpha` 与 `rank` 同量级较常见，过大可能导致不稳定。\n",
        "- 正确性自检：替换后、训练前准确率应与替换前接近（因 B=0）。\n",
        "- 训练对象：确保只优化 LoRA（A、B），其余参数保持冻结。\n",
        "- 部署：保存/加载 LoRA 增量参数即可，推理时与原模型叠加。\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 保存与加载 LoRA 参数（Adapter）\n",
        "\n",
        "- 保存：仅保存 LoRA 分支（A、B、alpha），体积小，便于分发与部署。\n",
        "- 加载：在同样完成 LoRA 替换后，将保存的参数拷回对应 `LoRALayer` 中即可生效。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存与加载 LoRA 参数的实用函数\n",
        "# 说明：\n",
        "# - 命名约定使用模块层级名，避免顺序不一致导致加载错位。\n",
        "# - 保存内容包含每个 LoRALayer 的 A、B、alpha。\n",
        "\n",
        "import torch\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def iter_lora_named_modules(model: torch.nn.Module):\n",
        "    \"\"\"遍历模型中所有 LoRALayer，返回 (模块全名, 模块对象)\"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LoRALayer):\n",
        "            yield name, module\n",
        "\n",
        "\n",
        "def save_lora_adapter(model: torch.nn.Module, filepath: str) -> None:\n",
        "    state: Dict[str, torch.Tensor] = {}\n",
        "    for name, m in iter_lora_named_modules(model):\n",
        "        state[f\"{name}.A\"] = m.A.detach().cpu()\n",
        "        state[f\"{name}.B\"] = m.B.detach().cpu()\n",
        "        # 将 alpha 一并存为张量，便于跨设备\n",
        "        state[f\"{name}.alpha\"] = torch.tensor(m.alpha)\n",
        "    torch.save(state, filepath)\n",
        "    print(f\"LoRA adapter 已保存到: {filepath}（包含 {len([k for k in state if k.endswith('.A')])} 个 LoRA 层）\")\n",
        "\n",
        "\n",
        "def load_lora_adapter(model: torch.nn.Module, filepath: str, map_location: str = \"cpu\") -> None:\n",
        "    state = torch.load(filepath, map_location=map_location)\n",
        "    restored, missing = 0, []\n",
        "    for name, m in iter_lora_named_modules(model):\n",
        "        keyA, keyB, keyAlpha = f\"{name}.A\", f\"{name}.B\", f\"{name}.alpha\"\n",
        "        if keyA in state and keyB in state and keyAlpha in state:\n",
        "            with torch.no_grad():\n",
        "                m.A.copy_(state[keyA].to(m.A.device))\n",
        "                m.B.copy_(state[keyB].to(m.B.device))\n",
        "                m.alpha = float(state[keyAlpha].item())\n",
        "            restored += 1\n",
        "        else:\n",
        "            missing.append(name)\n",
        "    print(f\"LoRA adapter 已加载。成功恢复 {restored} 层；缺失 {len(missing)} 层: {missing[:3]}{'...' if len(missing)>3 else ''}\")\n",
        "\n",
        "# 示例：保存当前微调得到的 LoRA 参数\n",
        "save_lora_adapter(model, \"lora_adapter.pt\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 推理示例（微调后）\n",
        "\n",
        "- 以测试集样本作为演示，打印前若干条预测与真实标签。\n",
        "- 注意：此处仅用于直观感受微调效果，完整指标请参考上方评估单元。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 推理示例\n",
        "# 说明：\n",
        "# - 这里示范如何从 DataLoader 中取一个 batch，打印预测结果与真实标签对比。\n",
        "# - 具体前向逻辑由 `train_classifier_simple`/`calc_accuracy_loader` 内部定义的模型调用保持一致。\n",
        "\n",
        "import torch\n",
        "\n",
        "def predict_one_batch(data_loader, max_batches: int = 1):\n",
        "    model.eval()\n",
        "    shown = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # 兼容 previous_chapters 内部的批数据结构\n",
        "            # 典型为 (input_ids, labels) 或字典；如结构不同，请参考库实现做相应适配\n",
        "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "            logits = model(inputs)\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            for i in range(min(5, inputs.shape[0])):\n",
        "                print(f\"样本{i}: 预测={int(preds[i].item())}, 真实={int(labels[i].item())}\")\n",
        "            shown += 1\n",
        "            if shown >= max_batches:\n",
        "                break\n",
        "\n",
        "predict_one_batch(test_loader, max_batches=1)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 小结\n",
        "\n",
        "- 仅训练 LoRA 分支即可实现高效适配，训练与部署成本显著降低。\n",
        "- 保存/加载 LoRA adapter 便于在不同环境快速复用微调效果。\n",
        "- 若需导出为 HuggingFace 生态可用的 Adapter 形式，可参考 `peft`/`transformers` 集成方案。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
## 加载 LoRA 后的快速评估
## 加载 LoRA 后的快速评估
## 加载 LoRA 后的快速评估
